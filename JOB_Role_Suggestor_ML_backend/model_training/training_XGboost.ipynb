{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f50158",
   "metadata": {},
   "source": [
    " # Job Role Prediction: Final Model Training with XGBoost\n",
    " \n",
    " This is the definitive training notebook for our project. To achieve the highest possible accuracy, we are making two major upgrades:\n",
    " \n",
    " 1.  **Using a More Powerful Algorithm**: We are replacing `RandomForestClassifier` with `XGBoost` (`XGBClassifier`), a state-of-the-art gradient boosting library known for its high performance and accuracy.\n",
    " 2.  **Exhaustive Hyperparameter Tuning**: We will use a comprehensive parameter grid with 10-fold cross-validation (`cv=10`) to find the absolute best settings for our new model.\n",
    " \n",
    " ### Steps:\n",
    " 1.  **Install XGBoost**: Ensure the library is installed.\n",
    " 2.  **Load Data**: Import the final weighted dataset.\n",
    " 3.  **Prepare & Split Data**: Standard preprocessing steps.\n",
    " 4.  **Hyperparameter Tuning**: Run an extensive `GridSearchCV` on the `XGBClassifier`.\n",
    " 5.  **Train & Evaluate Final Model**: Train the best model and analyze its performance.\n",
    " 6.  **Save Model**: Export the final, optimized model for the API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24338a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c87638",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load the Final Weighted Dataset\n",
    " \n",
    " We will use the `job_skills_80_roles_weighted.csv` file. This dataset has the best structure for our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9191e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Dataset shape: (50000, 141)\n",
      "\n",
      "Job Role column has been label encoded for XGBoost.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Job_Role",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Job_Role_Encoded",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "fe902af0-2de4-4697-94ac-47f5b8edd042",
       "rows": [
        [
         "0",
         "Real Estate Agent",
         "61"
        ],
        [
         "1",
         "Network Engineer",
         "44"
        ],
        [
         "2",
         "Physician",
         "50"
        ],
        [
         "3",
         "Event Planner",
         "23"
        ],
        [
         "4",
         "Full-Stack Developer",
         "29"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Role</th>\n",
       "      <th>Job_Role_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Real Estate Agent</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Network Engineer</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Physician</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Event Planner</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Full-Stack Developer</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Job_Role  Job_Role_Encoded\n",
       "0     Real Estate Agent                61\n",
       "1      Network Engineer                44\n",
       "2             Physician                50\n",
       "3         Event Planner                23\n",
       "4  Full-Stack Developer                29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_PATH = r'..\\dataset\\job_skills_80_roles_weighted.csv'\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Error: Dataset not found at '{DATASET_PATH}'.\")\n",
    "    print(\"Please run the final data generation script first.\")\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # The LabelEncoder is necessary for XGBoost, which requires integer labels.\n",
    "    le = LabelEncoder()\n",
    "    df['Job_Role_Encoded'] = le.fit_transform(df['Job_Role'])\n",
    "    \n",
    "    # Keep track of the mapping from encoded label to original job role\n",
    "    label_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "    \n",
    "    print(\"\\nJob Role column has been label encoded for XGBoost.\")\n",
    "    display(df[['Job_Role', 'Job_Role_Encoded']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664ca2c",
   "metadata": {},
   "source": [
    "\n",
    " ## 3. Prepare and Split Data\n",
    " \n",
    " We separate our features (X) from our new encoded target (y).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159bb871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000 samples\n",
      "Testing set size: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# --- Define Features (X) and Target (y) ---\n",
    "X = df.drop(['Job_Role', 'Job_Role_Encoded'], axis=1)\n",
    "y = df['Job_Role_Encoded'] # Use the encoded column for training\n",
    "\n",
    "feature_list = list(X.columns)\n",
    "\n",
    "# --- Split the data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples\")\n",
    "print(f\"Testing set size: {len(X_test)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec9112",
   "metadata": {},
   "source": [
    "\n",
    " ## 4. Exhaustive Hyperparameter Tuning for XGBoost\n",
    " \n",
    " This is the most computationally intensive step. We are giving `GridSearchCV` a wide range of parameters to test with 10-fold cross-validation to find the optimal model configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdcaacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPU-accelerated hyperparameter tuning with XGBoost...\n",
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [05:25:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning complete.\n",
      "\n",
      "Best parameters found: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best cross-validation score: 0.9639\n"
     ]
    }
   ],
   "source": [
    "# --- Define the parameter grid for XGBoost ---\n",
    "# This grid is slightly adjusted for GPU training efficiency\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [5, 7],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# --- Initialize the GridSearchCV object for GPU ---\n",
    "# **THE KEY CHANGE IS HERE**: We add device=\"cuda\"\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        objective='multi:softprob', \n",
    "        eval_metric='mlogloss', \n",
    "        use_label_encoder=False, \n",
    "        random_state=42,\n",
    "        device=\"cuda\"  # This tells XGBoost to use the GPU\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    n_jobs=16, # Still use CPU cores for data handling\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "print(\"Starting GPU-accelerated hyperparameter tuning with XGBoost...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Hyperparameter tuning complete.\")\n",
    "\n",
    "# --- Get the best parameters ---\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest parameters found: {best_params}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a73f38",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train and Evaluate the Final XGBoost Model\n",
    " \n",
    " We will now train a new model using the best parameters found by the grid search and evaluate it on our unseen test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98750518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the final XGBoost model with the best parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:12:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model training complete.\n",
      "\n",
      "Final XGBoost Model Accuracy: 0.9632\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "           AI Ethics Specialist       1.00      1.00      1.00       139\n",
      "                     Accountant       1.00      1.00      1.00       121\n",
      "                        Actuary       1.00      1.00      1.00       129\n",
      "                      Architect       1.00      1.00      1.00       127\n",
      "                         Artist       1.00      1.00      1.00       133\n",
      "              Backend Developer       1.00      1.00      1.00       120\n",
      "                      Biologist       1.00      1.00      1.00       123\n",
      "           Blockchain Developer       1.00      1.00      1.00       134\n",
      "                           Chef       1.00      1.00      1.00       125\n",
      "                        Chemist       1.00      1.00      1.00       120\n",
      "                 Civil Engineer       1.00      1.00      1.00       121\n",
      "                 Cloud Engineer       1.00      1.00      1.00       121\n",
      "           Construction Manager       1.00      1.00      1.00       131\n",
      "          Cybersecurity Analyst       1.00      1.00      1.00       136\n",
      "                   Data Analyst       1.00      1.00      1.00       141\n",
      "                  Data Engineer       1.00      1.00      1.00       121\n",
      "                 Data Scientist       1.00      1.00      1.00       119\n",
      "   Database Administrator (DBA)       1.00      1.00      1.00       125\n",
      "                        Dentist       1.00      1.00      1.00       126\n",
      "                DevOps Engineer       1.00      1.00      1.00       127\n",
      "                      Economist       1.00      1.00      1.00       122\n",
      "            Electrical Engineer       1.00      1.00      1.00       124\n",
      "                    Electrician       0.53      0.55      0.54       128\n",
      "                  Event Planner       1.00      1.00      1.00       124\n",
      "              Financial Analyst       1.00      1.00      1.00       126\n",
      "                    Firefighter       1.00      1.00      1.00       121\n",
      "              Firmware Engineer       1.00      1.00      1.00       120\n",
      "                Fitness Trainer       1.00      1.00      1.00       126\n",
      "             Frontend Developer       1.00      1.00      1.00       122\n",
      "           Full-Stack Developer       1.00      1.00      1.00       130\n",
      "                 Game Developer       1.00      1.00      1.00       125\n",
      "                      Geologist       1.00      1.00      1.00       124\n",
      "               Graphic Designer       1.00      1.00      1.00       128\n",
      "                     HR Manager       1.00      1.00      1.00       126\n",
      "          IT Support Specialist       0.48      0.49      0.48       123\n",
      "              Investment Banker       1.00      1.00      1.00       122\n",
      "                     Journalist       1.00      1.00      1.00       120\n",
      "                         Lawyer       1.00      1.00      1.00       117\n",
      "                      Librarian       0.51      0.45      0.48       123\n",
      "      Machine Learning Engineer       1.00      1.00      1.00       126\n",
      "          Management Consultant       1.00      1.00      1.00       128\n",
      "              Marketing Manager       1.00      1.00      1.00       134\n",
      "            Mechanical Engineer       1.00      1.00      1.00       127\n",
      "           Mobile App Developer       1.00      1.00      1.00       127\n",
      "               Network Engineer       1.00      1.00      1.00       125\n",
      "             Operations Manager       1.00      1.00      1.00       120\n",
      "                      Paralegal       1.00      1.00      1.00       127\n",
      "             Penetration Tester       1.00      1.00      1.00       130\n",
      "                     Pharmacist       1.00      1.00      1.00       128\n",
      "             Physical Therapist       1.00      1.00      1.00       119\n",
      "                      Physician       1.00      1.00      1.00       125\n",
      "                          Pilot       0.50      0.48      0.49       119\n",
      "                        Plumber       0.47      0.46      0.47       121\n",
      "                 Police Officer       1.00      1.00      1.00       123\n",
      "         Product Manager (Tech)       1.00      1.00      1.00       126\n",
      "                      Professor       1.00      1.00      1.00       129\n",
      "     Project Manager (Non-Tech)       1.00      1.00      1.00       130\n",
      "                   Psychologist       1.00      1.00      1.00       127\n",
      "    Public Relations Specialist       1.00      1.00      1.00       127\n",
      "                    QA Engineer       1.00      1.00      1.00       129\n",
      "    Quantum Computing Scientist       1.00      1.00      1.00       123\n",
      "              Real Estate Agent       1.00      1.00      1.00       121\n",
      "               Registered Nurse       1.00      1.00      1.00       128\n",
      "              Robotics Engineer       1.00      1.00      1.00       128\n",
      "                  Sales Manager       1.00      1.00      1.00       125\n",
      "           Salesforce Developer       1.00      1.00      1.00       125\n",
      "              Security Engineer       1.00      1.00      1.00       120\n",
      "Site Reliability Engineer (SRE)       1.00      1.00      1.00       118\n",
      "                  Social Worker       1.00      1.00      1.00       125\n",
      "              Software Engineer       1.00      1.00      1.00       118\n",
      "            Solutions Architect       1.00      1.00      1.00       122\n",
      "                   Statistician       1.00      1.00      1.00       130\n",
      "           Supply Chain Manager       1.00      1.00      1.00       127\n",
      "          Systems Administrator       1.00      1.00      1.00       122\n",
      "                        Teacher       1.00      1.00      1.00       122\n",
      "                     Translator       1.00      1.00      1.00       119\n",
      "                 UX/UI Designer       1.00      1.00      1.00       124\n",
      "                  Urban Planner       1.00      1.00      1.00       122\n",
      "                   Veterinarian       1.00      1.00      1.00       118\n",
      "                         Welder       0.52      0.58      0.55       126\n",
      "\n",
      "                       accuracy                           0.96     10000\n",
      "                      macro avg       0.96      0.96      0.96     10000\n",
      "                   weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Initialize and Train the Final Model ---\n",
    "print(\"Training the final XGBoost model with the best parameters...\")\n",
    "final_model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- Make predictions and evaluate ---\n",
    "y_pred_encoded = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
    "print(f\"\\nFinal XGBoost Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# --- Display the Classification Report ---\n",
    "# We need to convert the encoded labels back to original job roles for the report\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "y_pred_labels = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3342a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Save the Final Model, Feature List, and Label Encoder\n",
    " \n",
    " We need to save the `LabelEncoder` as well, so our API can convert the numeric predictions back into job role names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c011cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized XGBoost model saved to: saved_model_xgboost\\xgb_job_predictor_20250717_081838.joblib\n",
      "Feature list saved to: saved_model_xgboost\\xgb_feature_list_20250717_081838.joblib\n",
      "Label encoder saved to: saved_model_xgboost\\xgb_label_encoder_20250717_081838.joblib\n",
      "\n",
      "Setup complete! The new model is ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create a directory to save the model ---\n",
    "MODEL_DIR = 'saved_model_xgboost'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "\n",
    "# --- Define file paths ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f'xgb_job_predictor_{timestamp}.joblib')\n",
    "FEATURES_PATH = os.path.join(MODEL_DIR, f'xgb_feature_list_{timestamp}.joblib')\n",
    "ENCODER_PATH = os.path.join(MODEL_DIR, f'xgb_label_encoder_{timestamp}.joblib')\n",
    "\n",
    "# --- Save the files ---\n",
    "joblib.dump(final_model, MODEL_PATH)\n",
    "joblib.dump(feature_list, FEATURES_PATH)\n",
    "joblib.dump(le, ENCODER_PATH)\n",
    "\n",
    "print(f\"Optimized XGBoost model saved to: {MODEL_PATH}\")\n",
    "print(f\"Feature list saved to: {FEATURES_PATH}\")\n",
    "print(f\"Label encoder saved to: {ENCODER_PATH}\")\n",
    "print(\"\\nSetup complete! The new model is ready.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
